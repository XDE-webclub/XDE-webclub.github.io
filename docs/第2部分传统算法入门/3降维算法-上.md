
## 降维算法

作为一名数据科学家，我们手上的数据有非常多的特征。

虽然这听起来有利于建立更强大精准的模型，但它们有时候反倒也是建模中的一大难题。

怎样才能从1000或2000个变量里找到最重要的变量呢？

这种情况下降维算法及其他算法，如决策树，随机森林，PCA，因子分析，相关矩阵，和缺省值比例等，就能帮我们解决难题。


```python 
from sklearn.decomposition import PCA
import numpy as np

# 创建一些示例数据
X = np.array([[1, 2, 3],
              [2, 3, 4],
              [3, 4, 5],
              [4, 5, 6]])  # 特征矩阵

# 创建PCA降维模型
n_components = 2  # 指定要降维到的维度
model = PCA(n_components=n_components)

# .fit_transform()方法可以拟合数据，同时进行降维
X_reduced = model.fit_transform(X)

print("原始数据形状:", X.shape)
print("降维后数据形状:", X_reduced.shape)
print("降维后数据:")
print(X_reduced)
```


### 简单示例


```python 
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
# 加载数据
iris = load_iris()
iris_X = iris.data
iris_y = iris.target

# 创建PCA降维模型
# 分别降维到2维和3维
model2 = PCA(n_components=2)
model3 = PCA(n_components=3)

# 拟合模型并进行降维
X_reduced2 = model2.fit_transform(iris_X)
X_reduced3 = model3.fit_transform(iris_X)

print(X_reduced2)
print(X_reduced3)
```


### 效果评估


```python 
print("原始数据形状:", iris_X.shape)
print("降维后数据形状:", X_reduced2.shape)
print("降维后数据形状:", X_reduced3.shape)
print("降维后数据:")
print(X_reduced2)
print(X_reduced3)
```

