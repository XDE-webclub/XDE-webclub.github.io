
## 逻辑回归

假设你的一个朋友让你回答一道题。

可能的结果只有两种：你答对了或没有答对。

为了研究你最擅长的题目领域，你做了各种领域的题目。

那么这个研究的结果可能是这样的：

如果是一道十年级的三角函数题，你有70%的可能性能解出它。

但如果是一道五年级的历史题，你会的概率可能只有30%。

逻辑回归就是给你这样的概率结果。

一个具体的案例是这样的，如果三个月的雨天数为 3、5 和 8，而这三个月的销售数量分别为 8、12 和 18，回归算法会使用以下方程式将这些因子关联起来：

销售数量 = 2*（雨天数）+ 2

预测未知值
对于未知值，软件使用方程式进行预测。如果您知道七月会下雨六天，该软件会将七月的销售值估计为 14。

逻辑回归是一种统计模型，它使用数学中的逻辑函数或 logit 函数作为 x 和 y 之间的方程式。Logit 函数将 y 映射为 x 的 sigmoid 函数。

![](https://d1.awsstatic.com/sigmoid.bfc853980146c5868a496eafea4fb79907675f44.png)
![](https://d1.awsstatic.com/S-curve.36de3c694cafe97ef4e391ed26a5cb0b357f6316.png)

多个解释变量会影响因变量的值。要对此类输入数据集建模，逻辑回归公式假设不同自变量之间存在线性关系。您可以修改 sigmoid 函数并按如下公式计算最终输出变量 

y = f(β0 + β1x1 + β2x2+… βnxn)

符号 β 表示回归系数。当您给它一个其中包含因变量和自变量的已知值的足够大的实验数据集时，logit 模型可以反向计算这些系数值。 




```python 
# 绘制逻辑回归的不同回归系数的sigmoid函数

import matplotlib.pyplot as plt
import numpy as np
 
 
def sigmoid(x,p=1):
    # 直接返回sigmoid函数
    return 1. / (1. + np.exp(-p*x))
 
 
def plot_sigmoid(p=1):
    # param:起点，终点，间距
    x = np.arange(-8, 8, 0.2)
    y = sigmoid(x,p)
    plt.plot(x, y)
    plt.show()
 
 
if __name__ == '__main__':
    plot_sigmoid()
    plot_sigmoid(20)
    plot_sigmoid(0.5)
```


```python 
import numpy as np
from sklearn.linear_model import LogisticRegression

# 创建一些示例数据
X = np.array([[1], [2], [3], [4], [5]])  # 自变量
y = np.array([0, 0, 1, 1, 1])  # 因变量，0表示负类，1表示正类

# 创建逻辑回归模型
model = LogisticRegression()

# .fit()方法用于拟合模型，即训练模型x
model.fit(X, y)

# 预测新数据点
new_data_point = np.array([[6]])  # 要预测的新数据点
# .predict()方法预测新数据点的类别
predicted_class = model.predict(new_data_point)
# .predict_proba()方法预测新数据点的概率
predicted_probability = model.predict_proba(new_data_point)

print("预测类别:", predicted_class)
print("预测概率 (负类, 正类):", predicted_probability)
print(type(predicted_probability))
predicted_probability
```


### 简单示例


```python 
from sklearn import datasets
from sklearn.model_selection import train_test_split  
from sklearn.preprocessing import StandardScaler  
from sklearn.linear_model import LogisticRegression


datasets

# 加载数据集
digits = datasets.load_digits()  
  
# 获取特征和目标变量  
X = digits.data  
y = digits.target  
  
# 数据预处理：随机分割训练集和测试集 , 如果不指定 random_state，每次运行结果都不一样。42为约定俗成的随机数种子
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 数据标准化  
scaler = StandardScaler()  
# .fit_transform()方法先拟合数据，再标准化。和降维算法的语法一致
X_train = scaler.fit_transform(X_train)  
# .transform()方法直接使用在测试集上进行标准化操作
X_test = scaler.transform(X_test)  
  
# 创建Logistic Regression模型  , 如果不指定 random_state，每次运行结果都不一样。42为约定俗成的随机数种子
model = LogisticRegression(random_state=42)  
  
# .fit()方法用于拟合模型，即训练模型
model.fit(X_train, y_train)  
  
# .predict()方法预测新数据点的类别
y_pred = model.predict(X_test)  

```

