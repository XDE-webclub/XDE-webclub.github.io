---
sidebar_position: 2
---

## seq2seq模型与注意力机制Attention

seq2seq是一个很强大的模型，它的另一个常见的名字是编码-解码器（Encoder-Decoder）结构。

不但可以用来做机器翻译，还可以用来做很多NLP任务，比如自动摘要、对话系统等。

叫seq2seq的原因，在于其输入和输出都是一个词序列。那就是sequence to sequence，即seq2seq。

- 编码器 Encoder：读取输入文本，将输入的文本（词序列）整体编码成一个表示向量，而后交给Decoder进行解码。在编码器中，输入的词会变成一个one-hot向量，而后经过Embedding层投射到一个（512维的）向量空间，随后输入到RNN的结构里面去（这个RNN一般是LSTM，而且可以是双向等结构，而且可以有很多层，即把神经网络搞的很深，从而可以去提取更高级的特征）。

- Context Vector：经过了编码器中的RNN的一系列计算，输出来的对源文本整体的一个表示向量。

- 解码器Decoder：接受编码器编码出的输入（以及标准答案的词序列），自行生成文字。

Encoder RNN负责对源语言进行编码，学习源语言的隐含特征。Encoder RNN的最后一个神经元的隐状态作为Decoder RNN的初始隐状态。

1. 训练时：接受编码器给出的表示向量和“标准答案”作为输入。参考下图，根据第一个输入`<start>`符号和编码器送来的表示向量，先生成了预测结果$$\hat{y_1}$$。而后，根据第二个输入符号$$y_1$$（而不是前一步的预测结果$$\hat{y_1}$$），我们生成预测结果$$\hat{y_2}$$.

2. 很好理解，那么损失函数就是前一步最终生成的整个词表的概率分布与标准答案对应的one-hot向量的交叉熵函数，我们基于这个损失函数对RNN网络做梯度下降，从而让模型预测的越来越准。
