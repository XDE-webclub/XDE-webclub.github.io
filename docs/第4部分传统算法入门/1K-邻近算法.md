
## K-邻近算法

这个算法既可以解决分类问题，也可以用于回归问题，但工业上用于分类的情况更多。 

KNN先记录所有已知数据，再利用一个距离函数，

找出已知数据中距离未知事件最近的K组数据，

最后按照这K组数据里最常见的类别预测该事件。


```python 
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# 创建一些示例数据
X = np.array([[1, 2], [2, 3], [2, 5], [3, 2], [3, 3], [4, 5]])  # 特征
y = np.array([0, 0, 1, 0, 1, 1])  # 目标标签

# 创建K-最近邻分类器
k = 3  # 选择K的值
model = KNeighborsClassifier(n_neighbors=k).fit(X, y)

# 预测新数据点
new_data_point = np.array([[3, 4]])  # 要预测的新数据点
predicted_class = model.predict(new_data_point)

print("预测类别:", predicted_class)
```


### 简单实战


```python 
from sklearn.neighbors import KNeighborsClassifier
import numpy as np
from sklearn.model_selection import train_test_split

# 导入鸢尾花数据库
from sklearn.datasets import load_iris

# 加载数据
iris = load_iris()
iris_X = iris.data
iris_y = iris.target
print(len(iris_X))
# # 获取前2条数据,从0开始到2结束,不包括2。写法1
# print(iris_X[0:2])

# # 获取前2条数据,从0开始到2结束,不包括2。写法2，省略0
# print(iris_X[:2])

# # 获取前2条数据,从0开始到2结束,不包括2。写法3，省略0
# print(iris_X[:2,:])

# # 获取前2条数据,从0开始到2结束,不包括2。写法4，省略0,只取第一列
# print(iris_X[:2,0])

# # 查看花的类别
# print(iris_y)
# # 查看花的数据
# print(iris_X)
# # 合在一起查看

# 把数据打乱，并分成测试数据和训练数据，测试数据的比例为30%
X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.1)
# 查看训练数据
# print(y_train)
# 实例化KNN分类器
knn = KNeighborsClassifier()
# 训练
knn.fit(X_train, y_train)
# 查看对数据的预测
print(knn.predict(X_test))
# 查看真实数据
print(y_test)
```


### 效果评估


```python 
right = 0
error = 0
for i in zip(knn.predict(X_test),y_test):
    #print(i)
    if i[0] == i[1]:
        right +=1
    else:
        error +=1
print(right,error)
print('正确率：{}%'.format(right/(right+error)*100))
```


### 效果评估的改进


```python 
print('正确率：{}%'.format(knn.score(X_test,y_test)*100))

# 正确率：100.0%
```

